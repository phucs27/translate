{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Label</th>\n",
       "      <th>ClassID</th>\n",
       "      <th>Relative_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>enoutput_02024_10_07_22_49_51_552246.wav</td>\n",
       "      <td>en</td>\n",
       "      <td>2</td>\n",
       "      <td>/en/enoutput_02024_10_07_22_49_51_552246.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>enoutput_10002024_10_07_22_49_51_552246.wav</td>\n",
       "      <td>en</td>\n",
       "      <td>2</td>\n",
       "      <td>/en/enoutput_10002024_10_07_22_49_51_552246.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>enoutput_1002024_10_07_22_49_51_552246.wav</td>\n",
       "      <td>en</td>\n",
       "      <td>2</td>\n",
       "      <td>/en/enoutput_1002024_10_07_22_49_51_552246.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enoutput_10052024_10_07_22_49_51_552246.wav</td>\n",
       "      <td>en</td>\n",
       "      <td>2</td>\n",
       "      <td>/en/enoutput_10052024_10_07_22_49_51_552246.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>enoutput_10102024_10_07_22_49_51_552246.wav</td>\n",
       "      <td>en</td>\n",
       "      <td>2</td>\n",
       "      <td>/en/enoutput_10102024_10_07_22_49_51_552246.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Filename Label  ClassID  \\\n",
       "0     enoutput_02024_10_07_22_49_51_552246.wav    en        2   \n",
       "1  enoutput_10002024_10_07_22_49_51_552246.wav    en        2   \n",
       "2   enoutput_1002024_10_07_22_49_51_552246.wav    en        2   \n",
       "3  enoutput_10052024_10_07_22_49_51_552246.wav    en        2   \n",
       "4  enoutput_10102024_10_07_22_49_51_552246.wav    en        2   \n",
       "\n",
       "                                     Relative_path  \n",
       "0     /en/enoutput_02024_10_07_22_49_51_552246.wav  \n",
       "1  /en/enoutput_10002024_10_07_22_49_51_552246.wav  \n",
       "2   /en/enoutput_1002024_10_07_22_49_51_552246.wav  \n",
       "3  /en/enoutput_10052024_10_07_22_49_51_552246.wav  \n",
       "4  /en/enoutput_10102024_10_07_22_49_51_552246.wav  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Prepare training data from Metadata file\n",
    "# ----------------------------\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "data_path = r'D:\\0_Workplace\\Project_Python\\audio_process\\data\\train'\n",
    "\n",
    "# Read metadata file\n",
    "metadata_file = r\"D:\\0_Workplace\\Project_Python\\audio_process\\data\\train\\audio_labels1.csv\"\n",
    "df = pd.read_csv(metadata_file)\n",
    "df.head()\n",
    "\n",
    "# Construct file path by concatenating fold and file name\n",
    "df['Relative_path'] = '/'+df['Label'].astype(str) + '/' + df['Filename'].astype(str)\n",
    "\n",
    "# Take relevant columns\n",
    "# df = df[['relative_path', 'classID']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, random_split\n",
    "from pre_processing_data import AudioUtil\n",
    "\n",
    "# ----------------------------\n",
    "# Sound Dataset\n",
    "# ----------------------------\n",
    "class SoundDS(Dataset):\n",
    "   def __init__(self, df, data_path):\n",
    "      self.df = df\n",
    "      self.data_path = str(data_path)\n",
    "      self.duration = 5000\n",
    "      self.sr = 44100\n",
    "      self.channel = 2\n",
    "      self.shift_pct = 0.4\n",
    "            \n",
    "   # ----------------------------\n",
    "   # Number of items in dataset\n",
    "   # ----------------------------\n",
    "   def __len__(self):\n",
    "      return len(self.df)    \n",
    "      \n",
    "   # ----------------------------\n",
    "   # Get i'th item in dataset\n",
    "   # ----------------------------\n",
    "   def __getitem__(self, idx):\n",
    "      # Absolute file path of the audio file - concatenate the audio directory with\n",
    "      # the relative path\n",
    "      audio_file = self.data_path + self.df.loc[idx, 'Relative_path']\n",
    "      # Get the Class ID\n",
    "      class_id = self.df.loc[idx, 'ClassID']\n",
    "\n",
    "      aud = AudioUtil.open(audio_file)\n",
    "      # Some sounds have a higher sample rate, or fewer channels compared to the\n",
    "      # majority. So make all sounds have the same number of channels and same \n",
    "      # sample rate. Unless the sample rate is the same, the pad_trunc will still\n",
    "      # result in arrays of different lengths, even though the sound duration is\n",
    "      # the same.\n",
    "      reaud = AudioUtil.resample(aud, self.sr)\n",
    "      rechan = AudioUtil.rechannel(reaud, self.channel)\n",
    "\n",
    "      dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\n",
    "      shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n",
    "      # sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None) #mel spetrogram\n",
    "      sgram = AudioUtil.spectro_gram(shift_aud) # mfcc\n",
    "      aug_sgram = AudioUtil.spectro_augment(sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n",
    "\n",
    "      return aug_sgram, class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "import torch\n",
    "\n",
    "myds = SoundDS(df, data_path)\n",
    "\n",
    "# Random split of 80:20 between training and validation\n",
    "num_items = len(myds)\n",
    "num_train = round(num_items * 0.8)\n",
    "num_val = num_items - num_train\n",
    "train_ds, val_ds = random_split(myds, [num_train, num_val])\n",
    "\n",
    "# Create training and validation data loaders\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "from audioclassifier import AudioClassifier\n",
    "from audioclassifierRNN import AudioClassifierRNN\n",
    "\n",
    "# Create the model and put it on the GPU if available\n",
    "# model = nn.DataParallel(AudioClassifier())\n",
    "model = nn.DataParallel(AudioClassifierRNN(40, 128, 2,3))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "# Check that it is on Cuda\n",
    "next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ----------------------------\n",
    "# # Training Loop\n",
    "# # ----------------------------\n",
    "# import torch.utils\n",
    "# import torch.utils.tensorboard\n",
    "\n",
    "\n",
    "# def training(model, train_dl, num_epochs):\n",
    "#     # Tensorboard\n",
    "#     writer = torch.utils.tensorboard.SummaryWriter()\n",
    "#     # Loss Function, Optimizer and Scheduler\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "#     scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001,\n",
    "#                                                 steps_per_epoch=int(len(train_dl)),\n",
    "#                                                 epochs=num_epochs,\n",
    "#                                                 anneal_strategy='linear')\n",
    "#     # Repeat for each epoch\n",
    "#     for epoch in range(num_epochs):\n",
    "#         running_loss = 0.0\n",
    "#         correct_prediction = 0\n",
    "#         total_prediction = 0\n",
    "\n",
    "#         # Repeat for each batch in the training set\n",
    "#         for i, data in enumerate(train_dl):\n",
    "#             # Get the input features and target labels, and put them on the GPU\n",
    "#             # print(data)\n",
    "#             inputs = data[0].to(device)\n",
    "#             labels = data[1].to(device)\n",
    "\n",
    "#             # Normalize the inputs\n",
    "#             inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "#             inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "#             # Zero the parameter gradients\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             # forward + backward + optimize\n",
    "#             outputs = model(inputs)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             scheduler.step()\n",
    "\n",
    "#             # Keep stats for Loss and Accuracy\n",
    "#             running_loss += loss.item()\n",
    "\n",
    "#             # Get the predicted class with the highest score\n",
    "#             _, prediction = torch.max(outputs,1)\n",
    "#             # Count of predictions that matched the target label\n",
    "#             correct_prediction += (prediction == labels).sum().item()\n",
    "#             total_prediction += prediction.shape[0]\n",
    "\n",
    "#             #if i % 10 == 0:    # print every 10 mini-batches\n",
    "#             #    print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10))\n",
    "        \n",
    "#         # Print stats at the end of the epoch\n",
    "#         num_batches = len(train_dl)\n",
    "#         avg_loss = running_loss / num_batches\n",
    "#         avg_acc = correct_prediction/total_prediction\n",
    "#         writer.add_scalar(\"Loss/train\", avg_loss, epoch)\n",
    "#         writer.add_scalar(\"Acc/train\", avg_acc, epoch)\n",
    "#         print(f'Epoch: {epoch}, Loss: {avg_loss:.3f}, Accuracy: {avg_acc:.3f}')\n",
    "#         # Save model\n",
    "#         torch.save(model.state_dict(), 'model_1310_2.pt')\n",
    "#     print('Finished Training')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Training Loop\n",
    "# ----------------------------\n",
    "import torch.utils\n",
    "import torch.utils.tensorboard\n",
    "\n",
    "\n",
    "def training(model, train_dl, num_epochs):\n",
    "    # Tensorboard\n",
    "    writer = torch.utils.tensorboard.SummaryWriter()\n",
    "    # Loss Function, Optimizer and Scheduler\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001,\n",
    "                                                steps_per_epoch=int(len(train_dl)),\n",
    "                                                epochs=num_epochs,\n",
    "                                                anneal_strategy='linear')\n",
    "    # Repeat for each epoch\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct_prediction = 0\n",
    "        total_prediction = 0\n",
    "\n",
    "        # Repeat for each batch in the training set\n",
    "        for i, data in enumerate(train_dl):\n",
    "            # Get the input features and target labels, and put them on the GPU\n",
    "            inputs = data[0].to(device)\n",
    "            labels = data[1].to(device)\n",
    "\n",
    "            # inputs shape: (batch_size, num_channels, height, width)\n",
    "            # You need to reshape it to (batch_size, sequence_length, input_size)\n",
    "            \n",
    "            batch_size, num_channels, height, width = inputs.shape\n",
    "\n",
    "            # Reshape inputs to (batch_size, height, num_channels * width)\n",
    "            inputs = inputs.view(batch_size, height, num_channels * width)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)  # Now inputs have correct shape for LSTM\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Keep stats for Loss and Accuracy\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Get the predicted class with the highest score\n",
    "            _, prediction = torch.max(outputs, 1)\n",
    "            # Count of predictions that matched the target label\n",
    "            correct_prediction += (prediction == labels).sum().item()\n",
    "            total_prediction += prediction.shape[0]\n",
    "\n",
    "\n",
    "            #if i % 10 == 0:    # print every 10 mini-batches\n",
    "            #    print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10))\n",
    "        \n",
    "        # Print stats at the end of the epoch\n",
    "        num_batches = len(train_dl)\n",
    "        avg_loss = running_loss / num_batches\n",
    "        avg_acc = correct_prediction/total_prediction\n",
    "        writer.add_scalar(\"Loss/train\", avg_loss, epoch)\n",
    "        writer.add_scalar(\"Acc/train\", avg_acc, epoch)\n",
    "        print(f'Epoch: {epoch}, Loss: {avg_loss:.3f}, Accuracy: {avg_acc:.3f}')\n",
    "        # Save model\n",
    "        torch.save(model.state_dict(), 'model_1310_2.pt')\n",
    "    print('Finished Training')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 40, got 860",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[124], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[123], line 42\u001b[0m, in \u001b[0;36mtraining\u001b[1;34m(model, train_dl, num_epochs)\u001b[0m\n\u001b[0;32m     39\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# forward + backward + optimize\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Now inputs have correct shape for LSTM\u001b[39;00m\n\u001b[0;32m     43\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     44\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Python3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python3\\Lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py:168\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataParallel.forward\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids:\n\u001b[1;32m--> 168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m chain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mbuffers()):\n\u001b[0;32m    171\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_device_obj:\n",
      "File \u001b[1;32mc:\\Python3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\0_Workplace\\Project_Python\\audio_process\\audioclassifierRNN.py:21\u001b[0m, in \u001b[0;36mAudioClassifierRNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# LSTM layer\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# Select the last time step output (many-to-one)\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     out \u001b[38;5;241m=\u001b[39m out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]  \u001b[38;5;66;03m# lấy output của time step cuối cùng\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python3\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:898\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    894\u001b[0m     c_zeros \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers \u001b[38;5;241m*\u001b[39m num_directions,\n\u001b[0;32m    895\u001b[0m                           max_batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[0;32m    896\u001b[0m                           dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    897\u001b[0m     hx \u001b[38;5;241m=\u001b[39m (h_zeros, c_zeros)\n\u001b[1;32m--> 898\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    899\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_batched:\n",
      "File \u001b[1;32mc:\\Python3\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:827\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\u001b[38;5;28mself\u001b[39m,  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m    823\u001b[0m                        \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[0;32m    824\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[0;32m    825\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[0;32m    826\u001b[0m                        ):\n\u001b[1;32m--> 827\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m    829\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    830\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m    831\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python3\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:246\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    243\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput must have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_input_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dimensions, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 246\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    247\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 40, got 860"
     ]
    }
   ],
   "source": [
    "num_epochs=20\n",
    "training(model, train_dl, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Phucs\\AppData\\Local\\Temp\\ipykernel_2408\\371859634.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_inf.load_state_dict(torch.load('model_1310_2.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00, Total items: 694\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# Inference\n",
    "# ----------------------------\n",
    "def inference (model, test_dl):\n",
    "    correct_prediction = 0\n",
    "    total_prediction = 0\n",
    "\n",
    "    # Disable gradient updates\n",
    "    with torch.no_grad():\n",
    "        for data in test_dl:\n",
    "            # Get the input features and target labels, and put them on the GPU\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            # Normalize the inputs\n",
    "            inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "            inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "            # Get predictions\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Get the predicted class with the highest score\n",
    "            _, prediction = torch.max(outputs,1)\n",
    "            # Count of predictions that matched the target label\n",
    "            correct_prediction += (prediction == labels).sum().item()\n",
    "            total_prediction += prediction.shape[0]\n",
    "        \n",
    "    acc = correct_prediction/total_prediction\n",
    "    print(f'Accuracy: {acc:.2f}, Total items: {total_prediction}')\n",
    "\n",
    "# Run inference on trained model with the validation set load best model weights\n",
    "model_inf = nn.DataParallel(AudioClassifier())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_inf = model_inf.to(device)\n",
    "model_inf.load_state_dict(torch.load('model_1310_2.pt'))\n",
    "model_inf.eval()\n",
    "\n",
    "inference(model_inf, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Predict with a single audio file\n",
    "# ----------------------------\n",
    "def predict(model, audio_file):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Load the audio file\n",
    "    aud = AudioUtil.open(audio_file)\n",
    "    reaud = AudioUtil.resample(aud, 44100)  # Ensure sampling rate is 44100 Hz\n",
    "    rechan = AudioUtil.rechannel(reaud, 2)  # Ensure stereo audio (2 channels)\n",
    "\n",
    "    # Pad or truncate the audio to 4 seconds (4000 ms)\n",
    "    dur_aud = AudioUtil.pad_trunc(rechan, 5000)\n",
    "\n",
    "    # Shift the audio for augmentation (can be skipped if not needed)\n",
    "    shift_aud = AudioUtil.time_shift(dur_aud, 0.4)\n",
    "\n",
    "    # Generate a Mel-Spectrogram\n",
    "    # sgram = AudioUtil.spectro_gram(shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n",
    "    sgram = AudioUtil.spectro_gram(shift_aud)\n",
    "\n",
    "\n",
    "    # Normalize the spectrogram\n",
    "    sgram_m, sgram_s = sgram.mean(), sgram.std()\n",
    "    sgram = (sgram - sgram_m) / sgram_s\n",
    "\n",
    "    # Add batch dimension since model expects a batch of inputs\n",
    "    sgram = sgram.unsqueeze(0)\n",
    "\n",
    "    # Move to the same device as the model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    sgram = sgram.to(device)\n",
    "\n",
    "    # Disable gradient calculation (since we're only doing inference)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(sgram)\n",
    "\n",
    "    # Get the predicted class with the highest score\n",
    "    _, prediction = torch.max(outputs, 1)\n",
    "\n",
    "    # Return the predicted class ID\n",
    "    return prediction.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class ID: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Phucs\\AppData\\Local\\Temp\\ipykernel_2408\\2445075331.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_inf.load_state_dict(torch.load('model.pt'))\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model_inf = nn.DataParallel(AudioClassifier())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_inf = model_inf.to(device)\n",
    "model_inf.load_state_dict(torch.load('model.pt'))\n",
    "\n",
    "audio_file = r\"D:\\0_Workplace\\data_audio\\tu_thu\\vi_5_2024_10_13_22_33_00_847800.wav\"\n",
    "# Predict for a single audio file\n",
    "prediction = predict(model_inf, audio_file)\n",
    "print(f'Predicted class ID: {prediction}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Phucs\\AppData\\Local\\Temp\\ipykernel_2408\\1340821960.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_inf.load_state_dict(torch.load('model_1310.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enoutput_17252024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_17302024_10_07_22_49_51_552246.wav ==> Predicted class ID: 3\n",
      "enoutput_17352024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_17402024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_17452024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_17502024_10_07_22_49_51_552246.wav ==> Predicted class ID: 3\n",
      "enoutput_17552024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_17602024_10_07_22_49_51_552246.wav ==> Predicted class ID: 1\n",
      "enoutput_17652024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_17702024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_17752024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_17802024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_17852024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_17902024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_17952024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_18002024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_18052024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_18102024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_18152024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_18202024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_18252024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_18302024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_18352024_10_07_22_49_51_552246.wav ==> Predicted class ID: 3\n",
      "enoutput_18402024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_18452024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_18502024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_18552024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_18602024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_18652024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_18702024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_18752024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_18802024_10_07_22_49_51_552246.wav ==> Predicted class ID: 3\n",
      "enoutput_18852024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_18902024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_18952024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_19002024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_19052024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_19102024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_19152024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_19202024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_19252024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_19302024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_19352024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_19402024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_19452024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_19502024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_22352024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_22402024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_22452024_10_07_22_49_51_552246.wav ==> Predicted class ID: 3\n",
      "enoutput_22502024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_22552024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_22602024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_22652024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_22702024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_22752024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_22802024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_22852024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_22902024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_22952024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_23002024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_23052024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_23102024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_23152024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_23202024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_23252024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_23302024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_23352024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_23402024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_23452024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_23502024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_23552024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_23602024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_23652024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_23702024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_23752024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_23802024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_23852024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_23902024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_23952024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_24002024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_24052024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_24102024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_24152024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_24202024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_24252024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_24302024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_24352024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_24402024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_24452024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_26102024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_26152024_10_07_22_49_51_552246.wav ==> Predicted class ID: 3\n",
      "enoutput_26202024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_26252024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_26302024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_26352024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_26402024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_26452024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_26502024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_26552024_10_07_22_49_51_552246.wav ==> Predicted class ID: 3\n",
      "enoutput_26602024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_26652024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_26702024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_26752024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_26802024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_26852024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_26902024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_26952024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_27002024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_27052024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_27102024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_27152024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_27202024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_27252024_10_07_22_49_51_552246.wav ==> Predicted class ID: 2\n",
      "enoutput_27302024_10_07_22_49_51_552246.wav ==> Predicted class ID: 0\n",
      "count_vi:  1\n",
      "count_en:  105\n",
      "count_ko:  7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folder_path = r\"D:\\0_Workplace\\data_audio\\test_en\"\n",
    "model_inf = nn.DataParallel(AudioClassifier())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_inf = model_inf.to(device)\n",
    "model_inf.load_state_dict(torch.load('model_1310.pt'))\n",
    "count_vi = 0\n",
    "count_en = 0\n",
    "count_ko = 0\n",
    "\n",
    "for audio_file in os.listdir(folder_path):\n",
    "    if audio_file.endswith(\".wav\"):  # Chỉ xử lý file .wav\n",
    "        audio_file_path = os.path.join(folder_path, audio_file)\n",
    "\n",
    "        prediction = predict(model_inf, audio_file_path)\n",
    "        if prediction == 1:\n",
    "            count_vi = count_vi + 1\n",
    "        if prediction == 2:\n",
    "            count_en = count_en + 1\n",
    "        if prediction == 3:\n",
    "            count_ko = count_ko + 1\n",
    "        print(f'{audio_file} ==> Predicted class ID: {prediction}')\n",
    "print(f\"count_vi:  {count_vi}\")\n",
    "print(f\"count_en:  {count_en}\")\n",
    "print(f\"count_ko:  {count_ko}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
